<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>MOSS-Speech | Towards True Speech-to-Speech Models</title>
    <meta name="description" content="MOSS-Speech: A true speech-to-speech foundation model without text guidance. Direct speech understanding and generation, preserving paralinguistic cues with low latency and high expressivity." />
    <link rel="icon" href="assets/favicon.svg" type="image/svg+xml" />
    <link rel="preconnect" href="https://fonts.googleapis.com" />
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600;700&display=swap" rel="stylesheet" />
    <link rel="stylesheet" href="assets/style.css" />
  </head>
  <body>
    <header class="site-header">
      <div class="container header-inner">
        <div class="brand">
          <img src="assets/logo.svg" alt="Logo" class="logo" />
          <span class="brand-name">MOSS-Speech</span>
        </div>
        <nav class="nav">
          <a href="#features">Features</a>
          <a href="#demos">Demos</a>
          <a href="#intro">Model</a>
          <a href="https://github.com/" target="_blank" rel="noreferrer">GitHub</a>
        </nav>
      </div>
    </header>

    <main>
      <section class="hero">
        <div class="container">
          <div class="hero-title">
            <h1>MOSS-Speech: Towards True Speech-to-Speech Models Without Text Guidance</h1>
            <div class="title-divider"></div>
          </div>
          <div class="hero-inner">
            <div class="hero-copy">
              <p class="subtitle">MOSS-Speech is a true speech-to-speech large language model that directly understands and generates speech without relying on text intermediates, preserving paralinguistic cues while reducing latency and enhancing expressivity.</p>
              <div class="cta-row">
              <a class="btn primary" href="#demos">Try Demos</a>
              <a class="btn" href="#features">Features</a>
              </div>
            </div>
            <div class="hero-art">
              <img src="assets/hero-image.jpeg" alt="MOSS-Speech Architecture" class="hero-image" />
              <div class="glow"></div>
            </div>
          </div>
        </div>
      </section>

      <section id="features" class="section">
        <div class="container">
          <h2>Highlights</h2>
          <div class="features-grid">
            <div class="feature">
              <div class="emoji">ðŸŽ¯</div>
              <h3>True Speech-to-Speech</h3>
              <p>First LLM that directly processes speech without text intermediates, achieving SOTA performance while preserving paralinguistic cues.</p>
            </div>
            <div class="feature">
              <div class="emoji">ðŸ”§</div>
              <h3>Layer-Split Architecture</h3>
              <p>Modality-based layer splitting with frozen pre-training preserves text reasoning while adding native speech capabilities.</p>
            </div>
            <div class="feature">
              <div class="emoji">ðŸ“Š</div>
              <h3>Comprehensive Validation</h3>
              <p>Extensive experiments demonstrate superior cross-modal alignment and maintained text performance across benchmarks.</p>
            </div>
          </div>
        </div>
      </section>

      <section id="intro" class="section">
        <div class="container">
          <h2>Model Overview</h2>
          <div class="toc">
            <a href="#intro">Overview</a>
            <a href="#arch">Model Architecture</a>
            <a href="#data">Data Collection</a>
            <a href="#pretrain">Two-stage Pre-training</a>
            <a href="#sft">Supervised Fine-tuning</a>
          </div>
          <div class="intro-card">
            <p>
              Spoken dialogue systems often rely on cascaded pipelines that transcribe, process, and resynthesize speech. While effective, this design discards paralinguistic cues and limits expressivity. Recent end-to-end methods reduce latency and better preserve these cues, yet still rely on text intermediates, creating a fundamental bottleneck. We present MOSS-Speech, a true speech-to-speech large language model that directly understands and generates speech without relying on text guidance.
            </p>
            <p>
              Our approach combines a modality-based layer-splitting architecture with a frozen pre-training strategy, preserving the reasoning and knowledge of pretrained text LLMs while adding native speech capabilities. Experiments show that our model achieves state-of-the-art results in spoken question answering and delivers comparable speech-to-speech performance relative to existing text-guided systems, while still maintaining competitive text performance. By narrowing the gap between text-guided and direct speech generation, our work establishes a new paradigm for expressive and efficient end-to-end speech interaction.
            </p>
          </div>

          <h3 id="arch">Model Architecture</h3>
          <div class="intro-card">
            <p>
              We introduce a modalityâ€‘based layerâ€‘splitting Transformer that shares lower/middle layers for speechâ€“text fusion and splits the top layers into textâ€‘ and speechâ€‘specific branches (split at block 32 in a 36â€‘layer model), leveraging early layers for joint representation while reserving the final layers for modalityâ€‘specific generationâ€”thereby preserving the text LLMâ€™s reasoning ability and adding native, highâ€‘quality speech generation.
            </p>
            <img class="section-figure" src="assets/illustrations/architecture.png" alt="Model architecture with shared and split layers" />
          </div>

          <h3 id="data">Data Collection and Processing</h3>
          <div class="intro-card">
            <p>
              We begin with ~9M hours of realâ€‘world audio collected from the internet. A custom pyannoteâ€‘based voice activity detection (VAD) pipeline removes nonâ€‘speech segments, yielding ~4M hours of speech. The data are organized into two source types: (i) interleaved speechâ€“text preâ€‘training, drawn primarily from podcasts; and (ii) unsupervised speech preâ€‘training, drawn primarily from video content. We then apply CTCâ€‘based alignment to build interleaved sequences and, to boost knowledge density, synthesize additional interleaved data from highâ€‘quality text corpora via TTS.
            </p>
            <img class="section-figure" src="assets/illustrations/data-pipeline.png" alt="Data collection and processing pipeline" />
          </div>

          <h3 id="pretrain">Two-stage Pre-training</h3>
          <div class="intro-card">
            <ul>
              <li><b>Stage 1 â€” Speech alignment with a frozen text backbone.</b> We freeze all parameters of the Qwenâ€‘3â€‘8B backbone and train only the newly added speech components: speech token embeddings, speechâ€‘specific Transformer layers, and the speech LM head. This stage initializes speech parameters and establishes stable alignment to the pretrained text representations. Training runs for ~1 epoch with AdamW (cosine LR), initial LR 4eâ€‘4, batch ~2.2M tokens, weight decay 0.1, and context length 14,336.</li>
              <li><b>Stage 2 â€” Joint training with text knowledge preservation.</b> We expand trainable scope to enable crossâ€‘modal adaptation, evaluating three variants: (i) unfreeze the entire model for joint training; (ii) unfreeze only shared layers while keeping text embeddings, textâ€‘specific layers, and the text LM head frozen; (iii) progressively unfreeze shared layers from last to first. To prevent erosion of textual ability, we mix in a small portion of highâ€‘quality textâ€‘only preâ€‘training data during this stage.</li>
            </ul>
            <img class="section-figure" src="assets/illustrations/pretraining-stages.png" alt="Two-stage pre-training schedule" />
          </div>

          <h3 id="sft">Supervised Fine-tuning</h3>
          <div class="intro-card">
            <p>
              Because highâ€‘quality supervised data for speech assistants are scarce in the wild, we synthesize them from openâ€‘source text SFT corpora. We first adapt questionâ€“answer pairs with the GPTâ€‘5 API into speechâ€‘friendly formats (e.g., removing nonâ€‘vocal content and normalizing structures), then render the adapted text into audio with multiple TTS systems. We apply automatic quality filtering with SenseVoiceâ€‘Small ASR to discard lowâ€‘fidelity items. This pipeline yields over 1.5M QA pairs in total, including ~650k English and ~860k Chinese pairs.
            </p>
            <p>
              Fineâ€‘tuning mixes four I/O modes (speechâ†’speech, speechâ†’text, textâ†’speech, textâ†’text) under a unified prompting scheme, strengthening crossâ€‘modal alignment while maintaining text competence.
            </p>
            <img class="section-figure" src="assets/illustrations/sft-pipeline.png" alt="Supervised fine-tuning data and modes" />
          </div>
        </div>
      </section>

      <section id="demos" class="section">
        <div class="container">
          <div class="section-head">
            <h2>Demo </h2>
            <p class="note-inline">Model inference segments in the videos have been time-accelerated.</p>
            <div class="filters">
              <button class="chip is-active" data-filter="all">All</button>
              <button class="chip" data-filter="zh">Chinese</button>
              <button class="chip" data-filter="en">English</button>
            </div>
          </div>
          <div id="demo-grid" class="demo-grid" aria-live="polite"></div>
        </div>
      </section>

    </main>

    <footer class="site-footer">
      <div class="container footer-inner">
        <span>Â© <span id="year"></span> MOSS-Speech</span>
      </div>
    </footer>

    <script src="assets/app.js"></script>
  </body>
  </html>


